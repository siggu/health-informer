from bs4 import BeautifulSoup
import json
from typing import Optional
from openai import OpenAI
from pydantic import BaseModel, Field
import os
import uuid
from dotenv import load_dotenv
import sys

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# BaseCrawler import
from base.base_crawler import BaseCrawler


# Pydantic ëª¨ë¸ ì •ì˜ - í‘œì¤€ ìŠ¤í‚¤ë§ˆ
class HealthSupportInfo(BaseModel):
    """ê±´ê°• ì§€ì› ì •ë³´ í‘œì¤€ ìŠ¤í‚¤ë§ˆ"""

    id: str = Field(description="ê³ ìœ  ID (UUID)")
    title: str = Field(description="ê³µê³ /ì‚¬ì—…/í”„ë¡œê·¸ë¨ì˜ ì œëª©(í•œ ì¤„)")
    support_target: str = Field(
        description="ì§€ì› ëŒ€ìƒ ë˜ëŠ” ì‹ ì²­/ì°¸ê°€ ìê²©ì„ ê°„ê²°íˆ ìš”ì•½"
    )
    support_content: str = Field(description="ì§€ì› ë‚´ìš©/í˜œíƒ/ì§€ì› í•­ëª©ì„ í•µì‹¬ë§Œ ìš”ì•½")
    raw_text: Optional[str] = Field(
        default=None, description="ì›ë³¸ í…ìŠ¤íŠ¸ - êµ¬ì¡°í™” ì „ í¬ë¡¤ë§í•œ ì›ë³¸ ë°ì´í„°"
    )
    source_url: Optional[str] = Field(default=None, description="ì¶œì²˜ URL")
    region: Optional[str] = Field(default=None, description="ì§€ì—­ëª… (ì˜ˆ: ê´‘ì§„êµ¬, ì „êµ­)")


# LLM ì‘ë‹µìš© ë‚´ë¶€ ëª¨ë¸ (2ê°€ì§€ ì¼€ì´ìŠ¤ë¡œ ë¶„ë¦¬)
# 1. (ê¸°ì¡´) ë‹¨ë… ì‹¤í–‰ ì‹œ LLMì´ ì œëª©ê¹Œì§€ ì°¾ì•„ì•¼ í•˜ëŠ” ê²½ìš°
class _LLMResponseWithTitle(BaseModel):
    """LLM ì‘ë‹µìš© (ì œëª© í¬í•¨)"""

    title: str = Field(description="ê³µê³ /ì‚¬ì—…/í”„ë¡œê·¸ë¨ì˜ ì œëª©(í•œ ì¤„)")
    support_target: str = Field(
        description="ì§€ì› ëŒ€ìƒ ë˜ëŠ” ì‹ ì²­/ì°¸ê°€ ìê²©ì„ ê°„ê²°íˆ ìš”ì•½"
    )
    support_content: str = Field(description="ì§€ì› ë‚´ìš©/í˜œíƒ/ì§€ì› í•­ëª©ì„ í•µì‹¬ë§Œ ìš”ì•½")


# 2. (ì‹ ê·œ) ì›Œí¬í”Œë¡œìš°ì—ì„œ ì œëª©ì„ ë¯¸ë¦¬ ì•Œë ¤ì£¼ëŠ” ê²½ìš°
class _LLMResponseNoTitle(BaseModel):
    """LLM ì‘ë‹µìš© (ì œëª© ì œì™¸)"""

    support_target: str = Field(
        description="ì§€ì› ëŒ€ìƒ ë˜ëŠ” ì‹ ì²­/ì°¸ê°€ ìê²©ì„ ê°„ê²°íˆ ìš”ì•½"
    )
    support_content: str = Field(description="ì§€ì› ë‚´ìš©/í˜œíƒ/ì§€ì› í•­ëª©ì„ í•µì‹¬ë§Œ ìš”ì•½")


class LLMStructuredCrawler(BaseCrawler):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” í¬ë¡¤ëŸ¬"""

    def __init__(self, api_key: str = None, model: str = "gpt-4o"):
        """
        Args:
            api_key: OpenAI API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´)
            model: ì‚¬ìš©í•  ëª¨ë¸ (gpt-4o, gpt-4o-mini ë“±)
        """
        super().__init__()  # BaseCrawler ì´ˆê¸°í™”

        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•˜ì„¸ìš”."
            )

        self.client = OpenAI(api_key=self.api_key)
        self.model = model

    def parse_html_file(self, file_path: str) -> BeautifulSoup:
        """ë¡œì»¬ HTML íŒŒì¼ íŒŒì‹±"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                html_content = f.read()
            return BeautifulSoup(html_content, "html.parser")
        except Exception as e:
            print(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None

    def _extract_text_content(
        self, soup: BeautifulSoup, max_chars: int = 200000
    ) -> str:
        """
        HTMLì—ì„œ ì£¼ìš” í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ (ë‚´ë¶€ í—¬í¼)
        - ë¶ˆí•„ìš”í•œ ìš”ì†Œ(nav, footer, sidebar ë“±) ì œê±°
        - ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ìš°ì„  ì¶”ì¶œ
        - í…Œì´ë¸” ë°ì´í„° êµ¬ì¡°í™”

        Args:
            soup: BeautifulSoup ê°ì²´
            max_chars: ìµœëŒ€ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 200,000ì = ì•½ 50,000 í† í°)

        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ê¸¸ì´ ì œí•œ ì ìš©)
        """
        # ë³µì‚¬ë³¸ ìƒì„± (ì›ë³¸ soup ìˆ˜ì • ë°©ì§€)
        soup_copy = BeautifulSoup(str(soup), "html.parser")

        # 1ï¸âƒ£ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        unwanted_selectors = [
            "nav",
            "header",
            "footer",
            ".sidebar",
            ".menu",
            ".navigation",
            "#nav",
            "#header",
            "#footer",
            ".ad",
            ".advertisement",
            "script",
            "style",
            "noscript",
            ".cookie-banner",
            ".popup",
        ]

        for selector in unwanted_selectors:
            for element in soup_copy.select(selector):
                element.decompose()

        # 2ï¸âƒ£ ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
        main_content_selectors = [
            "main",
            "#content",
            "#main",
            ".content",
            ".main-content",
            ".contentArea",
            ".content-area",
            "article",
            ".article",
            "[role='main']",
        ]

        content_area = None
        for selector in main_content_selectors:
            content_area = soup_copy.select_one(selector)
            if content_area:
                break

        # ë©”ì¸ ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ body ì „ì²´ ì‚¬ìš©
        if not content_area:
            content_area = soup_copy.find("body") or soup_copy

        # 3ï¸âƒ£ í…Œì´ë¸” ë°ì´í„° êµ¬ì¡°í™”
        text_parts = []

        # í…Œì´ë¸” ì²˜ë¦¬
        for table in content_area.find_all("table"):
            table_lines = ["[í‘œ ì‹œì‘]"]

            # í…Œì´ë¸” í—¤ë”
            headers = []
            for th in table.find_all("th"):
                th_text = th.get_text(strip=True)
                if th_text:
                    headers.append(th_text)

            if headers:
                table_lines.append(" | ".join(headers))
                table_lines.append("-" * (len(" | ".join(headers))))

            # í…Œì´ë¸” í–‰
            for row in table.find_all("tr"):
                cells = []
                for cell in row.find_all(["td", "th"]):
                    cell_text = cell.get_text(strip=True)
                    if cell_text:
                        cells.append(cell_text)

                if cells:
                    table_lines.append(" | ".join(cells))

            table_lines.append("[í‘œ ë]\n")

            # í…Œì´ë¸”ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì›ë³¸ì—ì„œ ì œê±°
            text_parts.append("\n".join(table_lines))
            table.decompose()

        # 4ï¸âƒ£ ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í…Œì´ë¸”ì€ ì´ë¯¸ ì œê±°ë¨)
        text = content_area.get_text(separator="\n", strip=True)

        # ë¹ˆ ì¤„ ì œê±° ë° ì •ë¦¬
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        general_text = "\n".join(lines)

        # 5ï¸âƒ£ í…Œì´ë¸” í…ìŠ¤íŠ¸ì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ ê²°í•©
        if text_parts:
            cleaned_text = general_text + "\n\n" + "\n\n".join(text_parts)
        else:
            cleaned_text = general_text

        # 6ï¸âƒ£ ê¸¸ì´ ì œí•œ ì ìš©
        if len(cleaned_text) > max_chars:
            print(
                f"    âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(cleaned_text):,}ì). {max_chars:,}ìë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤."
            )
            cleaned_text = (
                cleaned_text[:max_chars] + "\n\n[... í…ìŠ¤íŠ¸ê°€ ì˜ë ¸ìŠµë‹ˆë‹¤ ...]"
            )

        return cleaned_text

    def structure_with_llm(
        self,
        soup: BeautifulSoup,
        title: Optional[str] = None,  # ğŸ‘ˆ [ìˆ˜ì •] title íŒŒë¼ë¯¸í„° ì¶”ê°€
        use_structured_output: bool = True,
    ) -> HealthSupportInfo:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ BeautifulSoup ê°ì²´ì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  êµ¬ì¡°í™”

        Args:
            soup: í¬ë¡¤ë§í•œ BeautifulSoup ê°ì²´
            title: (ì„ íƒ) í˜ì´ì§€ì˜ í™•ì •ëœ ì œëª©. ì œê³µë˜ë©´ ì´ ì œëª©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            use_structured_output: OpenAI Structured Output ì‚¬ìš© ì—¬ë¶€
        """

        # 1. soupì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        raw_text = self._extract_text_content(soup)

        # 2. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (title ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°)
        if title:
            # --- 'title'ì´ ì œê³µëœ ê²½ìš° (ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‹¤í–‰) ---
            system_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ê³µê³ ë¬¸ì„ êµ¬ì¡°ì ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ë³´ì¡°ì ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” '{title}'(ì´)ë¼ëŠ” ì‚¬ì—…ì— ëŒ€í•œ ì›ë¬¸ì„ ì½ê³ , 'ì§€ì› ëŒ€ìƒ'ê³¼ 'ì§€ì› ë‚´ìš©'ì„ ìš”ì•½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ê·œì¹™:
- ì›ë¬¸ì— ê·¼ê±°í•´ ì‘ì„±í•˜ê³ , ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ'ìœ¼ë¡œ ê¸°ì¬í•´ ì£¼ì„¸ìš”.
- ì§€ì› ëŒ€ìƒê³¼ ì§€ì› ë‚´ìš©ì€ í•µì‹¬ë§Œ ìš”ì•½í•´ ì£¼ì„¸ìš” (ê¸¸ì–´ë„ 4~6ì¤„ ì´ë‚´).
- í¬ë§·ì€ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆì— ë§ì¶° 'support_target'ì™€ 'support_content'ë§Œ ë°˜í™˜í•´ ì£¼ì„¸ìš”."""

            user_prompt = f"""'{title}' ì‚¬ì—…ì— ëŒ€í•œ ì›ë¬¸ì…ë‹ˆë‹¤. 'ì§€ì› ëŒ€ìƒ'ê³¼ 'ì§€ì› ë‚´ìš©'ì„ ì¶”ì¶œí•´ ì£¼ì„¸ìš”:
================ RAW TEXT ================
{raw_text}
========================================="""

            response_model = _LLMResponseNoTitle  # ğŸ‘ˆ ì œëª©ì´ ì—†ëŠ” ì‘ë‹µ ëª¨ë¸

        else:
            # --- 'title'ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° (ë‹¨ë… ì‹¤í–‰) ---
            system_prompt = """ë„ˆëŠ” í•œêµ­ì–´ ê³µê³ ë¬¸ì„ êµ¬ì¡°ì ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ë³´ì¡°ì ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì›ë¬¸ì—ì„œ 'ì œëª©', 'ì§€ì› ëŒ€ìƒ(ìê²©)', 'ì§€ì› ë‚´ìš©'ì„ ê¼­ ë½‘ì•„ì£¼ì„¸ìš”.
ê·œì¹™:
- ì›ë¬¸ì— ê·¼ê±°í•´ ì‘ì„±í•˜ê³ , ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ'ìœ¼ë¡œ ê¸°ì¬í•´ ì£¼ì„¸ìš”.
- ì œëª©(title)ì€ ì›ë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì‚¬ì—…ëª…(H3, H4 ë“±)ì„ 1ê°œë§Œ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤.
- ì§€ì› ëŒ€ìƒê³¼ ì§€ì› ë‚´ìš©ì€ í•µì‹¬ë§Œ ìš”ì•½í•´ ì£¼ì„¸ìš” (ê¸¸ì–´ë„ 4~6ì¤„ ì´ë‚´).
- í¬ë§·ì€ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆì— ë§ì¶° ë°˜í™˜í•´ ì£¼ì„¸ìš”."""

            user_prompt = f"""ë‹¤ìŒ ì›ë¬¸ì—ì„œ 'ì œëª©', 'ì§€ì› ëŒ€ìƒ', 'ì§€ì› ë‚´ìš©'ì„ ì¶”ì¶œí•´ ì£¼ì„¸ìš”:
================ RAW TEXT ================
{raw_text}
========================================="""

            response_model = _LLMResponseWithTitle  # ğŸ‘ˆ ì œëª©ì´ í¬í•¨ëœ ì‘ë‹µ ëª¨ë¸

        # 3. LLM API í˜¸ì¶œ
        try:
            if use_structured_output:
                # Structured Output ì‚¬ìš© (ë” ì •í™•í•¨)
                completion = self.client.beta.chat.completions.parse(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    response_format=response_model,  # ğŸ‘ˆ ë™ì  ì‘ë‹µ ëª¨ë¸ ì ìš©
                    temperature=0.1,
                )
                response_data = completion.choices[0].message.parsed

            else:
                # ì¼ë°˜ JSON ëª¨ë“œ ì‚¬ìš© (í˜¸í™˜ì„±)
                completion = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.1,
                )
                result_json = json.loads(completion.choices[0].message.content)
                response_data = response_model(**result_json)

            # 4. ìµœì¢… HealthSupportInfo ê°ì²´ ì¡°ë¦½
            if title:
                # 'title'ì´ ì œê³µëœ ê²½ìš°, íŒŒë¼ë¯¸í„° 'title'ì„ ì£¼ì…
                return HealthSupportInfo(
                    id=str(uuid.uuid4()),
                    title=title,  # ğŸ‘ˆ ì œê³µëœ title ì‚¬ìš©
                    **response_data.model_dump(),
                    raw_text=raw_text,
                )
            else:
                # 'title'ì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°, LLMì˜ ì‘ë‹µ('title' í¬í•¨)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                return HealthSupportInfo(
                    id=str(uuid.uuid4()),
                    **response_data.model_dump(),  # ğŸ‘ˆ LLMì´ ì°¾ì€ title ì‚¬ìš©
                    raw_text=raw_text,
                )

        except Exception as e:
            print(f"LLM êµ¬ì¡°í™” ì‹¤íŒ¨: {e}")
            raise

    def crawl_and_structure(
        self,
        url: str = None,
        file_path: str = None,
        region: str = None,
        title: Optional[str] = None,  # ğŸ‘ˆ [ìˆ˜ì •] title íŒŒë¼ë¯¸í„° ì¶”ê°€
    ) -> HealthSupportInfo:
        """
        ì›¹í˜ì´ì§€ ë˜ëŠ” íŒŒì¼ì„ í¬ë¡¤ë§í•˜ê³  LLMìœ¼ë¡œ êµ¬ì¡°í™”

        Args:
            url: í¬ë¡¤ë§í•  URL
            file_path: ë¡œì»¬ HTML íŒŒì¼ ê²½ë¡œ
            region: ì§€ì—­ëª… (ì˜ˆ: "ê´‘ì§„êµ¬", "ì „êµ­")
            title: (ì„ íƒ) í˜ì´ì§€ì˜ í™•ì •ëœ ì œëª©.
        """
        # 1. HTML ê°€ì ¸ì˜¤ê¸°
        if url:
            soup = self.fetch_page(url)
            source_url = url
        elif file_path:
            soup = self.parse_html_file(file_path)
            source_url = file_path
        else:
            raise ValueError("url ë˜ëŠ” file_path ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")

        if not soup:
            raise ValueError("HTMLì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 2. LLMìœ¼ë¡œ êµ¬ì¡°í™” (soup ê°ì²´ì™€ titleì„ ì§ì ‘ ì „ë‹¬)
        # ğŸ‘ˆ [ìˆ˜ì •] titleì„ structure_with_llmìœ¼ë¡œ ì „ë‹¬
        structured_data = self.structure_with_llm(soup, title=title)

        # 3. ë©”íƒ€ ì •ë³´ ì„¤ì •
        structured_data.source_url = source_url
        if region:
            structured_data.region = region

        return structured_data

    def save_to_json(self, data: HealthSupportInfo, output_path: str):
        """êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(data.model_dump(), f, ensure_ascii=False, indent=2)
            print(f"[OK] ë°ì´í„°ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"[ERROR] íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def print_structured_data(self, data: HealthSupportInfo):
        """êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print(f"â–  ID: {data.id}")
        print(f"â–  ì œëª©: {data.title}")
        if data.region:
            print(f"â–  ì§€ì—­: {data.region}")
        print("=" * 80)

        if data.support_target:
            print("\nâ–  ì§€ì› ëŒ€ìƒ(ìê²©)")
            self._print_multiline(data.support_target, indent=1)

        if data.support_content:
            print("\nâ–  ì§€ì› ë‚´ìš©")
            self._print_multiline(data.support_content, indent=1)

        if data.source_url:
            print(f"\nâ–  ì¶œì²˜: {data.source_url}")

        print("\n" + "=" * 80)

    def _print_multiline(self, text: str, indent: int = 0):
        """ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸ë¥¼ ë“¤ì—¬ì“°ê¸°í•˜ì—¬ ì¶œë ¥"""
        prefix = "  " * indent
        lines = text.split("\n")
        for line in lines:
            if line.strip():
                print(f"{prefix}{line.strip()}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë‹¨ë… í…ŒìŠ¤íŠ¸ìš©)"""
    import argparse

    parser = argparse.ArgumentParser(
        description="LLMì„ ì‚¬ìš©í•˜ì—¬ ì˜ë£Œë¹„ ì§€ì› ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ê³  êµ¬ì¡°í™”í•©ë‹ˆë‹¤."
    )
    parser.add_argument("--url", type=str, help="í¬ë¡¤ë§í•  ì›¹í˜ì´ì§€ URL")
    parser.add_argument("--file", type=str, help="í¬ë¡¤ë§í•  ë¡œì»¬ HTML íŒŒì¼ ê²½ë¡œ")
    parser.add_argument(
        "--output",
        type=str,
        default="structured_output.json",
        help="ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="ì‚¬ìš©í•  OpenAI ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-4o-mini)",
    )

    args = parser.parse_args()

    # URL ë˜ëŠ” íŒŒì¼ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ
    if not args.url and not args.file:
        print("\n" + "=" * 80)
        print("LLM ê¸°ë°˜ ì˜ë£Œë¹„ ì§€ì› ì •ë³´ í¬ë¡¤ëŸ¬")
        print("=" * 80)
        print("\nì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")

        args.url = input("ì›¹í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        args.output = (
            input("ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: structured_output.json): ").strip()
            or "structured_output.json"
        )

    # LLM í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = LLMStructuredCrawler(model=args.model)

    print(f"\n{'=' * 80}")
    if args.url:
        print(f"ì²˜ë¦¬ ì¤‘: {args.url}")
    else:
        print(f"ì²˜ë¦¬ ì¤‘: {args.file}")
    print(f"{'=' * 80}")

    try:
        # í¬ë¡¤ë§ ë° êµ¬ì¡°í™”
        if args.url:
            # ğŸ‘ˆ [ìˆ˜ì •] main í•¨ìˆ˜ëŠ” title ì—†ì´ í˜¸ì¶œí•˜ë¯€ë¡œ, LLMì´ ìŠ¤ìŠ¤ë¡œ ì œëª©ì„ ì°¾ìŠµë‹ˆë‹¤.
            structured_data = crawler.crawl_and_structure(url=args.url, title=None)
        else:
            structured_data = crawler.crawl_and_structure(
                file_path=args.file, title=None
            )

        # ê²°ê³¼ ì¶œë ¥
        crawler.print_structured_data(structured_data)

        # JSON ì €ì¥
        crawler.save_to_json(structured_data, args.output)

        print(f"\n[ì™„ë£Œ] êµ¬ì¡°í™”ëœ ë°ì´í„°ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"[ERROR] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
