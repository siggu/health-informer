# app/crawling/dbsetup_pipeline.py
# ëª©ì : district, welfare, ehealth í¬ë¡¤ëŸ¬ â†’ DB ì—…ë¡œë“œ â†’ policy_id ê·¸ë£¨í•‘
# ì¤‘ê°„ JSON ì—†ì´, ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ documents/embeddingsì— ì‚½ìž…
# ì§„í–‰ë¥ (%) ì¶œë ¥ + pgvector ì•ˆì „ì‚½ìž… ë²„ì „

import os, sys, argparse, traceback
from datetime import datetime
import psycopg2
from psycopg2.extras import execute_values, Json
from openai import OpenAI
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë³´ì •
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from app.crawling.crawlers.district_crawler import DistrictCrawler
from app.crawling.crawlers.welfare_crawler import WelfareCrawler
from app.crawling.crawlers.ehealth_crawler import EHealthCrawler
from app.crawling.crawlers import run_all_crawlers as rac
from app.dao.db_policy import dbuploader_policy as dbuploader
from app.dao.db_policy import dbgrouper_policy as dbgrouper
from app.dao.utils_db import eprint, extract_sitename_from_url, get_weight

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìƒìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMB_DIM = 1024  # bge-m3-ko ê¸°ì¤€. ë°”ê¾¸ë©´ ì—¬ê¸°/DB ëª¨ë‘ ì¼ì¹˜ì‹œì¼œì•¼ í•¨.


def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
    return p


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìˆ˜ì§‘ í•¨ìˆ˜ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_district(urls, out_dir):
    all_data = []
    for url in urls:
        crawler = DistrictCrawler(output_dir=out_dir)
        summary = crawler.run(start_url=url, save_links=True, save_json=False, return_data=True)
        all_data.extend(summary.get("data", []))
    return all_data


def collect_welfare(out_dir, no_filter=False, max_items=None):
    crawler = WelfareCrawler(output_dir=out_dir)
    data = crawler.run_workflow(filter_health=not no_filter, max_items=max_items,
                                return_data=True, save_json=False)
    return data or []


def collect_ehealth(out_dir, categories=None, max_pages=None):
    crawler = EHealthCrawler(output_dir=out_dir)
    data = crawler.run_workflow(categories=categories, max_pages_per_category=max_pages,
                                return_data=True, save_json=False)
    return data or []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DB ìŠ¤í‚¤ë§ˆ ë³´ìž¥ (documents + embeddings/vector)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_pgvector(conn):
    with conn.cursor() as cur:
        cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
    conn.commit()


def ensure_documents_schema(conn):
    """dbuploader.ensure_documents_schema ì‚¬ìš© (eval_target/eval_content í¬í•¨)"""
    with conn.cursor() as cur:
        dbuploader.ensure_documents_schema(cur)
    conn.commit()


def ensure_embeddings_vector_schema(conn, table="embeddings", col="embedding", dim=EMB_DIM):
    """embeddingsê°€ ì—†ìœ¼ë©´ ìƒì„±, ìžˆìœ¼ë©´ embeddingì„ VECTOR(dim)ë¡œ ìœ ì§€"""
    with conn.cursor() as cur:
        # í…Œì´ë¸” ì—†ìœ¼ë©´ ìƒì„±
        cur.execute("""
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1 FROM information_schema.tables WHERE table_name = 'embeddings'
            ) THEN
                CREATE TABLE embeddings (
                    id         BIGSERIAL PRIMARY KEY,
                    doc_id     BIGINT NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
                    field      TEXT NOT NULL CHECK (field IN ('title','requirements','benefits')),
                    embedding  VECTOR(%s) NOT NULL,
                    created_at TIMESTAMPTZ DEFAULT NOW(),
                    UNIQUE (doc_id, field)
                );
                CREATE INDEX IF NOT EXISTS idx_embeddings_doc_field ON embeddings (doc_id, field);
            END IF;
        END$$;
        """, (dim,))
        # ì»¬ëŸ¼ íƒ€ìž…ì´ ë²¡í„°ì¸ì§€ í™•ì¸
        cur.execute("""
            SELECT udt_name, data_type
            FROM information_schema.columns
            WHERE table_name=%s AND column_name=%s
        """, (table, col))
        r = cur.fetchone()
        # ê¸°ì¡´ì— ë°°ì—´ ë“±ìœ¼ë¡œ ë˜ì–´ ìžˆìœ¼ë©´ ë²¡í„°ë¡œ ê°•ì œ ë³€í™˜
        if r and r[0] != "vector":
            cur.execute(f"""
                ALTER TABLE {table}
                ALTER COLUMN {col} TYPE vector(%s)
                USING (
                    CASE
                      WHEN {col} IS NULL THEN NULL
                      ELSE ( '[' || array_to_string({col}, ',') || ']' )::vector
                    END
                );
            """, (dim,))
    conn.commit()


def build_vector_literal(vec, dim=EMB_DIM):
    """íŒŒì´ì¬ list[float] -> pgvector ë¬¸ìžì—´ ë¦¬í„°ëŸ´"""
    if not vec:
        return None
    if len(vec) > dim:
        vec = vec[:dim]
    elif len(vec) < dim:
        vec = list(vec) + [0.0] * (dim - len(vec))
    parts = (f"{float(x):.7f}" for x in vec)
    return "[" + ",".join(parts) + "]"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DB ì—…ë¡œë“œ (ì§„í–‰ë¥  % í‘œì‹œ + eval_* ë°˜ì˜ + pgvector ì•ˆì „ì‚½ìž…)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ... (ìƒë‹¨ import/ìƒìˆ˜ ë™ì¼)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DB ì—…ë¡œë“œ (ì§„í–‰ë¥  % í‘œì‹œ + eval_* ë°˜ì˜ + pgvector ì•ˆì „ì‚½ìž…)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def upload_records(records, reset="none", emb_model="dragonkue/BGE-m3-ko", commit_every=50):
    if not records:
        eprint("[upload] ì—…ë¡œë“œí•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    preprocess_title = dbuploader.preprocess_title
    get_embedding = dbuploader.get_embedding

    load_dotenv()
    DB_URL = os.getenv("DATABASE_URL")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    if not DB_URL or not OPENAI_API_KEY:
        raise RuntimeError("DATABASE_URL, OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    _ = OpenAI(api_key=OPENAI_API_KEY)

    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()

    total = len(records)
    bar_length = 40

    try:
        # ìŠ¤í‚¤ë§ˆ ë³´ìž¥ (documentsì— eval_target/eval_content ì¶”ê°€ í¬í•¨)
        ensure_pgvector(conn)
        ensure_documents_schema(conn)
        ensure_embeddings_vector_schema(conn, table="embeddings", col="embedding", dim=EMB_DIM)

        if reset != "none":
            cur.execute("TRUNCATE TABLE embeddings, documents RESTART IDENTITY CASCADE;")
            conn.commit()
            print(f"âœ… í…Œì´ë¸” ë¦¬ì…‹ ì™„ë£Œ: {reset}")

        inserted = 0
        for idx, item in enumerate(records, 1):
            title = item.get("title","")
            requirements = item.get("support_target","")
            benefits = item.get("support_content","")
            raw_text = item.get("raw_text","")
            url = item.get("source_url","")
            region = item.get("region","")

            # NEW: 0~10 ì›ì‹œì ìˆ˜ + í•©ì„±ì ìˆ˜
            eval_target  = item.get("eval_target")
            eval_content = item.get("eval_content")

            sitename = extract_sitename_from_url(url)
            weight = get_weight(region, sitename)

            cur.execute("""
                INSERT INTO documents
                    (title, requirements, benefits, raw_text, url, policy_id,
                     region, sitename, weight, eval_target, eval_content,
                     llm_reinforced, llm_reinforced_sources)
                VALUES
                    (%s, %s, %s, %s, %s, %s,
                     %s, %s, %s, %s, %s, %s,
                     %s)
                RETURNING id;
            """, (
                title, requirements, benefits, raw_text, url, None,
                region, sitename, weight, eval_target, eval_content, 
                False, None
            ))
            doc_id = cur.fetchone()[0]

            emb_rows = []
            title_emb_text = preprocess_title(title)

            for fname, text_value in (("title", title_emb_text),
                                        ("requirements", requirements),
                                        ("benefits", benefits)):
                vec = get_embedding(text_value, emb_model)
                if vec:
                    lit = build_vector_literal(vec, EMB_DIM)   # â† ë¬¸ìžì—´ ë¦¬í„°ëŸ´ë¡œ ë³€í™˜
                    emb_rows.append((doc_id, fname, lit))

            if emb_rows:
                execute_values(
                    cur,
                    "INSERT INTO embeddings (doc_id, field, embedding) VALUES %s",
                    emb_rows,
                    template="(%s, %s, %s::vector)"           # â† ::vector ìºìŠ¤íŒ… í•„ìˆ˜
                )

            inserted += 1
            percent = (inserted / total) * 100
            filled = int(bar_length * percent / 100)
            bar = "â–ˆ" * filled + "-" * (bar_length - filled)
            sys.stdout.write(f"\r[Upload] |{bar}| {percent:6.2f}% ({inserted}/{total})")
            sys.stdout.flush()

            if inserted % commit_every == 0:
                conn.commit()

        conn.commit()
        print(f"\nðŸŽ‰ ì—…ë¡œë“œ ì™„ë£Œ! ì´ {inserted}ê±´ ì‚½ìž…")

    except Exception as e:
        conn.rollback()
        eprint(f"[upload] ì—ëŸ¬ë¡œ ë¡¤ë°±: {e}")
        raise
    finally:
        cur.close()
        conn.close()
        
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê·¸ë£¹í•‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def group_policies(threshold=0.85, batch_size=500, reset_all=False, verbose=True, unify=True):
    # 1) 1ì°¨ ê·¸ë£¨í•‘
    res = dbgrouper.assign_policy_ids(
        title_field="title",
        similarity_threshold=threshold,
        batch_size=batch_size,
        dry_run=False,
        reset_all_on_start=reset_all,
        verbose=verbose,
    )

    # 2) ë£¨íŠ¸ë¡œ policy_id í†µì¼ (ì˜µì…˜)
    if unify:
        from psycopg2 import connect
        from app.dao.db_policy.dbgrouper_policy import unify_policy_ids_after_grouping
        from app.dao.db_policy.dbgrouper_policy import _build_dsn_from_env  # ì´ë¯¸ ê·¸ íŒŒì¼ì— ìžˆìŒ

        dsn = _build_dsn_from_env()
        with connect(dsn) as conn:
            unify_policy_ids_after_grouping(conn)

    return res



def _get_runall_urls():
    for name in ["TARGET_URLS", "DISTRICT_TARGETS", "DEFAULT_URLS", "URLS"]:
        if hasattr(rac, name):
            v = getattr(rac, name)
            try:
                return list(v)
            except Exception:
                pass
    return []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    p = argparse.ArgumentParser(description="í†µí•© í¬ë¡¤ë§ â†’ DB ì—…ë¡œë“œ â†’ policy_id ê·¸ë£¨í•‘ (in-memory, ì§„í–‰ë¥  í‘œì‹œ, eval_* ë°˜ì˜)")
    p.add_argument("--source", choices=["district", "welfare", "ehealth", "all"], default="district")
    p.add_argument("--urls", nargs="*", help="district ì‹œìž‘ URLë“¤")
    p.add_argument("--out-dir", default=os.path.join(PROJECT_ROOT, "app", "crawling", "output"))
    p.add_argument("--reset", choices=["none", "truncate"], default="none")
    p.add_argument("--group", action="store_true")
    p.add_argument("--threshold", type=float, default=0.85)
    p.add_argument("--batch-size", type=int, default=500)
    p.add_argument("--use-runall-targets", action="store_true", help="district ìˆ˜ì§‘ ì‹œ run_all_crawlers.pyì˜ URL ëª©ë¡ ì‚¬ìš©")
    args = p.parse_args()

    _ensure_dir(args.out_dir)

    try:
        mem_data = []

        if args.source in ("district", "all"):
            if args.use_runall_targets:
                urls = _get_runall_urls()
                if not urls:
                    eprint("[district] run_all_crawlers.pyì—ì„œ URLì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. --urls ì¸ìžë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                    urls = args.urls or []
            else:
                urls = args.urls or [
                    "https://health.gangnam.go.kr/web/business/support/sub01.do",
                    "https://health.gangdong.go.kr/health/site/main/content/GD20030100",
                    "https://www.gangbuk.go.kr/health/main/contents.do?menuNo=400151",
                    "https://www.gangseo.seoul.kr/health/ht020231",
                    "https://www.gwanak.go.kr/site/health/05/10502010600002024101710.jsp",
                    "https://www.gwangjin.go.kr/health/main/contents.do?menuNo=300080",
                    "https://www.guro.go.kr/health/contents.do?key=1320&",
                    "https://www.dongjak.go.kr/healthcare/main/contents.do?menuNo=300342",
                    "https://www.sdm.go.kr/health/contents/infectious/law",
                    "https://www.seocho.go.kr/site/sh/03/10301000000002015070902.jsp",
                    "https://www.sb.go.kr/bogunso/contents.do?key=6553",
                    "https://www.ydp.go.kr/health/contents.do?key=6073&",
                    "https://www.songpa.go.kr/ehealth/contents.do?key=4525&",
                    "https://jongno.go.kr/Health.do?menuId=401309&menuNo=401309",
                ]

            eprint(f"[district] {len(urls)}ê°œ URL ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ìˆ˜ì§‘)")
            mem_data += collect_district(urls, args.out_dir)

        if args.source in ("welfare", "all"):
            mem_data += collect_welfare(args.out_dir)

        if args.source in ("ehealth", "all"):
            mem_data += collect_ehealth(args.out_dir)

        if not mem_data:
            eprint("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        eprint(f"[upload] ë©”ëª¨ë¦¬ ë ˆì½”ë“œ {len(mem_data)}ê±´ ì—…ë¡œë“œ ì¤‘â€¦")
        upload_records(mem_data, reset=args.reset)

        if args.group:
            eprint("[group] policy_id ê·¸ë£¨í•‘ ì‹œìž‘")
            result = group_policies(args.threshold, args.batch_size)
            print("[group result]", result)

        print("\nâœ… ì™„ë£Œ:", len(mem_data), "records")

    except Exception as e:
        traceback.print_exc()
        eprint(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
