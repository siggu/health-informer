# app/dao/db_policy/dbuploader_policy.py
# -*- coding: utf-8 -*-
import os
import sys
import json
import argparse
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë³´ì •
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
    
import app.dao.utils_db as utils_db
    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SentenceTransformers (dragonkue/BGE-m3-ko)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from sentence_transformers import SentenceTransformer
except Exception as e:
    print(f"[ERROR] sentence_transformers ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
    sys.exit(1)

load_dotenv()
DB_URL = os.getenv("DATABASE_URL")
if not DB_URL:
    utils_db.eprint("í™˜ê²½ë³€ìˆ˜ DATABASE_URLì´ í•„ìš”í•©ë‹ˆë‹¤.")
    sys.exit(1)

def build_argparser():
    p = argparse.ArgumentParser(description="êµ¬ì¡°í™” JSONì„ documents/embeddings í…Œì´ë¸”ì— ì ì¬í•˜ëŠ” ë¡œë”")
    p.add_argument("--file","-f", default="app/output/ebogun.json",
                   help="ì ì¬í•  JSON íŒŒì¼ ê²½ë¡œ (ë˜ëŠ” __demo__ ë¡œ ë°ëª¨ 1ê±´)")
    p.add_argument("--reset", choices=["none","truncate"], default="none",
                   help="ë¡œë”© ì „ì— í…Œì´ë¸” ë¦¬ì…‹ ë°©ì‹ (none|truncate). default: none")
    p.add_argument("--model", default="dragonkue/BGE-m3-ko",
                   help="ì„ë² ë”© ëª¨ë¸ëª… (default: dragonkue/BGE-m3-ko)")
    p.add_argument("--commit-every", type=int, default=50,
                   help="Nê°œ ë¬¸ì„œë§ˆë‹¤ ì»¤ë°‹ (default: 50)")
    return p

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ì²˜ë¦¬ / ì„ë² ë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_WS = re.compile(r"\s+")
def _normalize_space(s: str) -> str:
    return _WS.sub(" ", s).strip()

def preprocess_title(title: str) -> str:
    if not title: return ""
    t = str(title).replace("\n"," ").replace("\r"," ")
    return _normalize_space(t)

_ST_MODEL = None
def _load_st_model(model_name: str):
    global _ST_MODEL
    if _ST_MODEL is None:
        _ST_MODEL = SentenceTransformer(model_name)
    return _ST_MODEL

EMB_DIM = 1024  # pgvector(1024)
def _pad_or_truncate(vec, dim=EMB_DIM):
    if len(vec) == dim: return vec
    if len(vec) > dim:  return vec[:dim]
    return vec + [0.0]*(dim - len(vec))

def get_embedding(text: str, model_name: str):
    if not text or not str(text).strip():
        return None
    m = _load_st_model(model_name)
    v = m.encode(str(text).strip(), normalize_embeddings=False)
    v = v.tolist() if hasattr(v, "tolist") else list(v)
    v = _pad_or_truncate(v, EMB_DIM)
    return v

def _to_vector_literal(vec):
    # pgvector ì…ë ¥ í¬ë§·: '[v1, v2, ...]'
    return "[" + ", ".join(f"{float(x):.8f}" for x in vec) + "]"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìŠ¤í‚¤ë§ˆ ë³´ì¥ (documents / embeddings)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CREATE_DOCUMENTS_SQL = """
CREATE TABLE IF NOT EXISTS documents (
    id                       BIGSERIAL PRIMARY KEY,
    title                    TEXT,
    requirements             TEXT,
    benefits                 TEXT,
    raw_text                 TEXT,
    url                      TEXT,
    policy_id                BIGINT,
    region                   TEXT,
    sitename                 TEXT,
    weight                   NUMERIC,
    eval_target              INTEGER,
    eval_content             INTEGER,
    llm_reinforced           BOOLEAN DEFAULT FALSE,
    llm_reinforced_sources   JSONB,
    created_at               TIMESTAMPTZ DEFAULT NOW(),
    updated_at               TIMESTAMPTZ DEFAULT NOW()
);
"""

DOC_COLUMNS = {
    "title": "TEXT",
    "requirements": "TEXT",
    "benefits": "TEXT",
    "raw_text": "TEXT",
    "url": "TEXT",
    "policy_id": "BIGINT",
    "region": "TEXT",
    "sitename": "TEXT",
    "weight": "NUMERIC",
    "eval_target": "INTEGER",
    "eval_content": "INTEGER",
    "llm_reinforced": "BOOLEAN DEFAULT FALSE",
    "llm_reinforced_sources": "JSONB",
    "updated_at": "TIMESTAMPTZ DEFAULT NOW()",
}

def ensure_documents_schema(cur):
    cur.execute("""
    CREATE TABLE IF NOT EXISTS documents (
        id BIGSERIAL PRIMARY KEY,
        title TEXT,
        requirements TEXT,
        benefits TEXT,
        raw_text TEXT,
        url TEXT,
        policy_id BIGINT,
        region TEXT,
        sitename TEXT,
        weight NUMERIC,
        eval_target INTEGER,
        eval_content INTEGER,
        llm_reinforced BOOLEAN DEFAULT FALSE,
        llm_reinforced_sources JSONB,
        created_at TIMESTAMPTZ DEFAULT NOW(),
        updated_at TIMESTAMPTZ DEFAULT NOW()
    );
    """)
    cur.execute("CREATE INDEX IF NOT EXISTS idx_documents_policy ON documents(policy_id);")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_documents_region ON documents(region);")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_documents_url ON documents(url);")

    # ì»¬ëŸ¼ ë³´ê°•(ì¤‘ë³µ ì¶”ê°€ ë°©ì§€)
    for name, ddl in {
        "title": "TEXT",
        "requirements": "TEXT",
        "benefits": "TEXT",
        "raw_text": "TEXT",
        "url": "TEXT",
        "policy_id": "BIGINT",
        "region": "TEXT",
        "sitename": "TEXT",
        "weight": "NUMERIC",
        "eval_target": "INTEGER",
        "eval_content": "INTEGER",
        "llm_reinforced": "BOOLEAN DEFAULT FALSE",
        "llm_reinforced_sources": "JSONB",
        "updated_at": "TIMESTAMPTZ DEFAULT NOW()"
    }.items():
        cur.execute(f"ALTER TABLE documents ADD COLUMN IF NOT EXISTS {name} {ddl};")

    # íŠ¸ë¦¬ê±° í•¨ìˆ˜ ìƒì„±(ì¡´ì¬í•´ë„ ë®ì–´ì“°ê¸° OK)
    cur.execute("""
    CREATE OR REPLACE FUNCTION set_documents_updated_at()
    RETURNS TRIGGER AS $$
    BEGIN
        NEW.updated_at = NOW();
        RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;
    """)

    # âš ï¸ ì—¬ê¸°ì„œ 'IF NOT EXISTS'ë¥¼ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.
    cur.execute("SELECT 1 FROM pg_trigger WHERE tgname='trg_documents_updated_at';")
    exists = cur.fetchone() is not None
    if not exists:
        cur.execute("""
        CREATE TRIGGER trg_documents_updated_at
        BEFORE UPDATE ON documents
        FOR EACH ROW
        EXECUTE PROCEDURE set_documents_updated_at();
        """)

def ensure_embeddings_schema(cur):
    # vector íƒ€ì…/í™•ì¥ ìœ ë¬´ì™€ ë¬´ê´€í•˜ê²Œ, í…Œì´ë¸”ì€ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ìƒì„±
    cur.execute(f"""
    CREATE TABLE IF NOT EXISTS embeddings (
        id BIGSERIAL PRIMARY KEY,
        doc_id BIGINT REFERENCES documents(id) ON DELETE CASCADE,
        field TEXT,
        embedding vector(1024),
        created_at TIMESTAMPTZ DEFAULT NOW()
    );
    """)
    cur.execute("CREATE INDEX IF NOT EXISTS idx_embeddings_doc ON embeddings(doc_id);")

def reset_tables(cur, mode: str):
    if mode == "truncate":
        cur.execute("TRUNCATE TABLE embeddings, documents RESTART IDENTITY CASCADE;")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    args = build_argparser().parse_args()
    json_path = args.file
    reset_mode = args.reset
    model_name = args.model
    commit_every = max(1, args.commit_every)

    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    if json_path == "__demo__":
        data = [{
            "title": "ì„œìš¸ì‹œ 65ì„¸ ì´ìƒ ë…ê±°ì–´ë¥´ì‹  ë°©ë¬¸ê±´ê°•ê´€ë¦¬",
            "support_target": "ì„œìš¸ ê±°ì£¼ 65ì„¸ ì´ìƒ ë…ê±°ë…¸ì¸",
            "support_content": "ê°„í˜¸ì‚¬ ë°©ë¬¸ ê±´ê°•ìƒë‹´, ë§Œì„±ì§ˆí™˜ ê´€ë¦¬, ë³´ê±´ì†Œ ì—°ê³„",
            "raw_text": "ì„œìš¸ì‹œ ë°©ë¬¸ê±´ê°•ê´€ë¦¬ ì‚¬ì—… ì•ˆë‚´...",
            "source_url": "https://seoul.go.kr/health/elderly-care",
            "region": "ì„œìš¸íŠ¹ë³„ì‹œ",
            "eval_target": 8,
            "eval_content": 9
        }]
        print("ğŸ§ª ë°ëª¨ ëª¨ë“œë¡œ 1ê±´ì„ ì ì¬í•©ë‹ˆë‹¤.")
    else:
        if not os.path.exists(json_path):
            utils_db.eprint(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
            sys.exit(1)
        with open(json_path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                utils_db.eprint(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                sys.exit(1)

    print(f"ğŸ“¡ Connecting to {DB_URL}")
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()

    try:
        # ìŠ¤í‚¤ë§ˆ ë³´ì¥
        ensure_documents_schema(cur)
        ensure_embeddings_schema(cur)
        if reset_mode != "none":
            reset_tables(cur, reset_mode)
            conn.commit()
            print(f"âœ… í…Œì´ë¸” ë¦¬ì…‹ ì™„ë£Œ: {reset_mode}")

        inserted = 0
        total = len(data)

        for idx, item in enumerate(data, 1):
            title = item.get("title","")
            requirements = item.get("support_target","")
            benefits = item.get("support_content","")
            raw_text = item.get("raw_text","")
            url = item.get("source_url","")
            region = item.get("region","")

            eval_target = item.get("eval_target")     # 0~10 or None
            eval_content = item.get("eval_content")   # 0~10 or None

            policy_id = None
            sitename = utils_db.extract_sitename_from_url(url)
            weight = utils_db.get_weight(region, sitename) if hasattr(utils_db,"get_weight") else 0
            llm_reinforced = False
            llm_reinforced_sources = None

            # documents ì‚½ì…
            cur.execute("""
                INSERT INTO documents
                    (title, requirements, benefits, raw_text, url, policy_id,
                    region, sitename, weight, eval_target, eval_content,
                    llm_reinforced, llm_reinforced_sources)
                VALUES
                    (%s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s,
                    %s, %s)
                RETURNING id;
            """, (
                title, requirements, benefits, raw_text, url, None,
                region, sitename, weight,
                eval_target, eval_content, llm_reinforced, llm_reinforced_sources
            ))
            doc_id = cur.fetchone()[0]
            # âœ… policy_idì— ìì‹ ì˜ id ì„¤ì •
            cur.execute(
                "UPDATE documents SET policy_id = %s WHERE id = %s;",
                (doc_id, doc_id)
            )

            # Embeddings ìƒì„±: title / requirements / benefits
            emb_rows = []
            title_norm = preprocess_title(title)
            for field, text_value in (("title", title_norm),
                                      ("requirements", requirements),
                                      ("benefits", benefits)):
                vec = get_embedding(text_value, model_name)
                if vec:
                    emb_rows.append((doc_id, field, _to_vector_literal(vec)))

            if emb_rows:
                # embeddings(doc_id, field, embedding vector(1024))
                execute_values(
                    cur,
                    "INSERT INTO embeddings (doc_id, field, embedding) VALUES %s",
                    emb_rows,
                    template="(%s, %s, %s::vector)"
                )

            inserted += 1
            if inserted % commit_every == 0:
                conn.commit()
                print(f"ğŸ’¾ {inserted}/{total}ê°œ ë¬¸ì„œ ì»¤ë°‹ ì™„ë£Œ")

            print(f"âœ… Inserted document ({idx}/{total}): {title}")

        conn.commit()
        print(f"ğŸ‰ All data inserted successfully! ì´ {inserted}ê±´")

    except Exception as e:
        conn.rollback()
        utils_db.eprint(f"ì—ëŸ¬ ë°œìƒìœ¼ë¡œ ë¡¤ë°±í–ˆìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)
    finally:
        cur.close()
        conn.close()

if __name__ == "__main__":
    main()
