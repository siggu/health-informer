# -*- coding: utf-8 -*-
import os
import sys
import json
import argparse
import psycopg2
from psycopg2.extras import execute_values, Json
from openai import OpenAI
from dotenv import load_dotenv
import app.dao.utils_db as utils_db

# --------------------------------
# 1. ì¸ì íŒŒì„œ
# --------------------------------
def build_argparser():
    p = argparse.ArgumentParser(
        description="êµ¬ì¡°í™” JSONì„ documents/embeddings í…Œì´ë¸”ì— ì ì¬í•˜ëŠ” ë¡œë”"
    )
    p.add_argument(
        "--file", "-f",
        default="app/output/ebogun.json",
        help="ì ì¬í•  JSON íŒŒì¼ ê²½ë¡œ (default: app/output/ebogun.json)"
    )
    p.add_argument(
        "--reset",
        choices=["none", "truncate"],
        default="none",
        help="ë¡œë”© ì „ì— í…Œì´ë¸” ë¦¬ì…‹ ë°©ì‹ (none|truncate). default: none"
    )
    p.add_argument(
        "--model",
        default="text-embedding-3-small",
        help="ì„ë² ë”© ëª¨ë¸ëª… (default: text-embedding-3-small)"
    )
    p.add_argument(
        "--commit-every",
        type=int,
        default=50,
        help="Nê°œ ë¬¸ì„œë§ˆë‹¤ ì»¤ë°‹ (default: 50)"
    )
    return p

# --------------------------------
# 2. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# --------------------------------
load_dotenv()
DB_URL = os.getenv("DATABASE_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not DB_URL:
    utils_db.eprint("í™˜ê²½ë³€ìˆ˜ DATABASE_URLì´ í•„ìš”í•©ë‹ˆë‹¤.")
    sys.exit(1)
if not OPENAI_API_KEY:
    utils_db.eprint("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    sys.exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

# -------------------------------
# 3. ìŠ¤í‚¤ë§ˆ ë³´ê°•
# -------------------------------
ALTER_DOCUMENTS_SQL = """
ALTER TABLE documents
    ADD COLUMN IF NOT EXISTS title TEXT,
    ADD COLUMN IF NOT EXISTS requirements TEXT,
    ADD COLUMN IF NOT EXISTS benefits TEXT,
    ADD COLUMN IF NOT EXISTS raw_text TEXT,
    ADD COLUMN IF NOT EXISTS url TEXT,
    ADD COLUMN IF NOT EXISTS policy_id BIGINT,
    ADD COLUMN IF NOT EXISTS region TEXT,
    ADD COLUMN IF NOT EXISTS sitename TEXT,
    ADD COLUMN IF NOT EXISTS weight INTEGER DEFAULT 0,
    ADD COLUMN IF NOT EXISTS eval_scores JSONB,
    ADD COLUMN IF NOT EXISTS eval_overall INTEGER,
    ADD COLUMN IF NOT EXISTS llm_reinforced BOOLEAN DEFAULT FALSE,
    ADD COLUMN IF NOT EXISTS llm_reinforced_sources JSONB,
    ADD COLUMN IF NOT EXISTS updated_at TIMESTAMPTZ DEFAULT NOW();
"""

def ensure_documents_schema(cur):
    cur.execute(ALTER_DOCUMENTS_SQL)

# -------------------------------
# 4. ì „ì²˜ë¦¬ í•¨ìˆ˜
# -------------------------------
def preprocess_title(title: str) -> str:
    """ì œëª© ì„ë² ë”© ê°•ê±´í™”ë¥¼ ìœ„í•´ ì›ë¬¸ + ë„ì–´ì“°ê¸° ì œê±° ë²„ì „ ë³‘í•©"""
    if not title:
        return ""
    no_space = title.replace(" ", "")
    return f"{title.strip()} {no_space}"

# --------------------------------
# 5. ì„ë² ë”© í•¨ìˆ˜
# --------------------------------
def get_embedding(text: str, model: str):
    if not text or text.strip() == "":
        return None
    resp = client.embeddings.create(
        model=model,
        input=text.replace("\n", " ")
    )
    return resp.data[0].embedding

# --------------------------------
# 6. í…Œì´ë¸” ë¦¬ì…‹
# --------------------------------
def reset_tables(cur, mode: str):
    """
    mode == 'truncate' ì¸ ê²½ìš°:
      - ì™¸ë˜í‚¤ë¥¼ ê³ ë ¤í•´ embeddings â†’ documents ìˆœìœ¼ë¡œ TRUNCATE
      - RESTART IDENTITY + CASCADE
    """
    if mode == "truncate":
        cur.execute("TRUNCATE TABLE embeddings, documents RESTART IDENTITY CASCADE;")

# --------------------------------
# 7. ë©”ì¸ ë¡œì§
# --------------------------------
def main():
    args = build_argparser().parse_args()

    json_path = args.file
    reset_mode = args.reset
    model_name = args.model
    commit_every = max(1, args.commit_every)

    if not os.path.exists(json_path):
        utils_db.eprint(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        sys.exit(1)

    with open(json_path, "r", encoding="utf-8") as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError as e:
            utils_db.eprint(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            sys.exit(1)

    # DB ì—°ê²°
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()

    try:
        # ìŠ¤í‚¤ë§ˆ ë³´ê°• + ì„ íƒì  ë¦¬ì…‹
        ensure_documents_schema(cur)
        if reset_mode != "none":
            reset_tables(cur, reset_mode)
            conn.commit()
            print(f"âœ… í…Œì´ë¸” ë¦¬ì…‹ ì™„ë£Œ: {reset_mode}")

        inserted = 0

        for idx, item in enumerate(data, 1):
            # llm_crawler.py ì‚°ì¶œ(í‘œì¤€ í‚¤)
            title = item.get("title", "")
            requirements = item.get("support_target", "")
            benefits = item.get("support_content", "")
            raw_text = item.get("raw_text", "")
            url = item.get("source_url", "")
            region = item.get("region", "")

            # NEW: í‰ê°€ í•„ë“œ
            eval_scores = item.get("eval_scores")
            eval_overall = item.get("eval_overall")

            # ë¶€ê°€ í•„ë“œ
            policy_id = None
            sitename = utils_db.extract_sitename_from_url(url)
            weight = utils_db.get_weight(region, sitename) if hasattr(utils_db, "get_weight") else 0
            llm_reinforced = False
            llm_reinforced_sources = None

            # documents ì‚½ì…
            cur.execute(
                """
                INSERT INTO documents
                    (title, requirements, benefits, raw_text, url, policy_id,
                     region, sitename, weight, eval_scores, eval_overall,
                     llm_reinforced, llm_reinforced_sources)
                VALUES
                    (%s, %s, %s, %s, %s, %s,
                     %s, %s, %s, %s, %s,
                     %s, %s)
                RETURNING id;
                """,
                (
                    title, requirements, benefits, raw_text, url, policy_id,
                    region, sitename, weight, Json(eval_scores) if eval_scores is not None else None,
                    eval_overall, llm_reinforced, llm_reinforced_sources
                )
            )
            doc_id = cur.fetchone()[0]

            # --- title ì „ì²˜ë¦¬ í›„ ì„ë² ë”© ---
            title_modified = preprocess_title(title)

            # ê° í•„ë“œë³„ ì„ë² ë”© ìƒì„±
            emb_rows = []
            for fname, text_value in (
                ("title", title_modified),
                ("requirements", requirements),
                ("benefits", benefits),
            ):
                vec = get_embedding(text_value, model_name)
                if vec:
                    emb_rows.append((doc_id, fname, vec))

            # ì¼ê´„ ì‚½ì…
            if emb_rows:
                execute_values(
                    cur,
                    "INSERT INTO embeddings (doc_id, field, embedding) VALUES %s",
                    emb_rows,
                    template="(%s, %s, %s)"
                )

            inserted += 1

            # ì£¼ê¸°ì ìœ¼ë¡œ ì»¤ë°‹
            if inserted % commit_every == 0:
                conn.commit()
                print(f"ğŸ’¾ {inserted}ê°œ ë¬¸ì„œ ì»¤ë°‹ ì™„ë£Œ")

            print(f"âœ… Inserted document ({idx}/{len(data)}): {title}")

        conn.commit()
        print(f"ğŸ‰ All data inserted successfully! ì´ {inserted}ê±´")

    except Exception as e:
        conn.rollback()
        utils_db.eprint(f"ì—ëŸ¬ ë°œìƒìœ¼ë¡œ ë¡¤ë°±í–ˆìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)
    finally:
        cur.close()
        conn.close()

if __name__ == "__main__":
    main()
