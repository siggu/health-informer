# app/langgraph/nodes/persist_pipeline.py
# -*- coding: utf-8 -*-
"""
persist_pipeline.py

ì„¸ì…˜ ì¢…ë£Œ ì‹œ:
  1) Cleaner: ë©”ì‹œì§€ PII ë§ˆìŠ¤í‚¹ / no_store ì •ì±… / ê¸¸ì´ ì œí•œ
  2) Summarizer: rolling_summary + ë©”ì‹œì§€ ê¸°ë°˜ ìµœì¢… ìš”ì•½ ìƒì„±
  3) DiffMerger: ephemeral_profile / ephemeral_collection â†” DB ë³‘í•©
  4) Vectorizer: dragonkue/bge-m3-ko ì„ë² ë”© ìƒì„±
  5) Persister: profiles / collections(íŠ¸ë¦¬í”Œ) / conversations / messages /
                conversation_embeddings ë¥¼ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜ìœ¼ë¡œ upsert/insert

ì£¼ì˜:
  - Policy DB(documents/embeddings)ëŠ” ì—¬ê¸°ì„œ ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤. (ì¡°íšŒ ì „ìš©)
  - collections ìŠ¤í‚¤ë§ˆëŠ” íŠ¸ë¦¬í”Œ ê¸°ë°˜:
      collections(
        id BIGINT PK,
        profile_id BIGINT,
        subject TEXT,
        predicate TEXT,
        object TEXT,
        code_system TEXT,
        code TEXT
      )
"""

from __future__ import annotations

import os
from typing import Any, Dict, List, Optional, TypedDict, Literal
from datetime import datetime, timezone

from dotenv import load_dotenv

load_dotenv()

import psycopg  # psycopg3
from sentence_transformers import SentenceTransformer

from app.langgraph.utils.cleaner_utils import clean_messages
from app.dao import db_user_utils


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ (Cleaner ê¸°ë³¸ê°’ ë° DB / ì„ë² ë”© ëª¨ë¸)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENV_ENABLE = os.getenv("PERSIST_ENABLE_CLEANER", "true").lower() == "true"
ENV_MODE: Literal["full", "mask-only", "off"] = os.getenv("PERSIST_CLEANER_MODE", "full").lower()
ENV_NO_STORE_POLICY: Literal["drop", "redact"] = os.getenv("PERSIST_NO_STORE_POLICY", "redact").lower()

DB_URL = os.getenv("DATABASE_URL")
if DB_URL and DB_URL.startswith("postgresql+psycopg://"):
    DB_URL = DB_URL.replace("postgresql+psycopg://", "postgresql://", 1)

# ì„ë² ë”© ëª¨ë¸ ì´ë¦„ (ìš°ì„ ìˆœìœ„: CONV_EMB_MODEL > EMBEDDING_MODEL > ê¸°ë³¸ê°’)
EMBED_MODEL_NAME = (
    os.getenv("CONV_EMB_MODEL")
    or os.getenv("EMBEDDING_MODEL")
    or "dragonkue/bge-m3-ko"
)

# lazy ë¡œë”©ìš© ì „ì—­
_EMB_MODEL: Optional[SentenceTransformer] = None


def _get_embedding_model() -> SentenceTransformer:
    global _EMB_MODEL
    if _EMB_MODEL is None:
        _EMB_MODEL = SentenceTransformer(EMBED_MODEL_NAME)
    return _EMB_MODEL


def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


class Message(TypedDict, total=False):
    role: Literal["user", "assistant", "tool"]
    content: str
    created_at: str
    meta: Dict[str, Any]


class PersistResult(TypedDict, total=False):
    ok: bool
    conversation_id: Optional[str]
    counts: Dict[str, int]
    warnings: List[str]


def _append_tool(msgs: List[Message], text: str, meta: Optional[Dict[str, Any]] = None) -> Message:
    """
    msgs ë¦¬ìŠ¤íŠ¸ì— tool ë¡œê·¸ 1ê°œë¥¼ append í•˜ê³ , ê·¸ Messageë¥¼ ë°˜í™˜.
    - persist ë‚´ë¶€ì—ì„œëŠ” cleaned(ì‹¤ì œ DB ì €ì¥ìš©)ì—ë§Œ ì¶”ê°€í•˜ê³ 
      ê·¸ë˜í”„ë¡œ ë¦¬í„´í•  delta ë¦¬ìŠ¤íŠ¸ì—ëŠ” ë°˜í™˜ê°’ì„ ë”°ë¡œ ëª¨ì€ë‹¤.
    """
    msg: Message = {
        "role": "tool",
        "content": text,
        "created_at": _now_iso(),
        "meta": meta or {},
    }
    msgs.append(msg)
    return msg


def _parse_median_income_ratio(raw: Any) -> Optional[float]:
    if raw is None:
        return None
    s = str(raw).strip()
    if not s:
        return None

    # "120%" â†’ "120"
    if s.endswith("%"):
        s = s[:-1].strip()

    try:
        v = float(s)
    except ValueError:
        return None

    # 0~10 ì‚¬ì´ëŠ” ë¹„ìœ¨(ë°°)ë¡œ ë³´ê³ , ê·¸ ì´ìƒì€ í¼ì„¼íŠ¸ë¡œ ë³¸ë‹¤
    # 1.2 â†’ 1.2  /  120 â†’ 1.2
    if v <= 10:
        return v
    else:
        return v / 100.0


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Summarizer (ê°„ë‹¨ ë²„ì „)
#  - rolling_summary + ìµœê·¼ user ë©”ì‹œì§€ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìš”ì•½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _summarize_session(rolling_summary: Optional[str], messages: List[Message]) -> str:
    last_user = next((m["content"] for m in reversed(messages) if m.get("role") == "user"), "")
    base = rolling_summary or ""
    return f"[SUMMARY]\nprev={base[:120]}\nlast_user={last_user[:120]}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Vectorizer (dragonkue/bge-m3-ko ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _embed_chunks(text: str) -> List[Dict[str, Any]]:
    """
    ì„¸ì…˜ ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ 1ê°œ chunkë¡œ ë³´ê³  dragonkue/bge-m3-ko ì„ë² ë”© ìƒì„±.
    - ë°˜í™˜ í˜•ì‹: [{"chunk_id": str, "embedding": List[float]}]
    - DB ìŠ¤í‚¤ë§ˆì˜ VECTOR ì°¨ì›(CONV_EMB_DIM)ê³¼ ëª¨ë¸ ì°¨ì›ì„ ë§ì¶°ì•¼ í•¨.
    """
    if not text:
        return []

    model = _get_embedding_model()
    # bge ê³„ì—´ ê¶Œì¥: normalize_embeddings=True (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ìš©)
    vec = model.encode([text], normalize_embeddings=True)[0]
    emb_list = vec.tolist()  # numpy.ndarray â†’ list[float]

    return [{
        "chunk_id": "full",
        "embedding": emb_list,
    }]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DiffMerger: ephemeral_profile / ephemeral_collection â†” DB
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _merge_profile(ephemeral: Dict[str, Any], db_profile: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ì„ì‹œ í”„ë¡œí•„ê³¼ DB í”„ë¡œí•„ ë³‘í•©.
    - ephemeral ê°’ì´ ìˆìœ¼ë©´ ìš°ì„  ì ìš©
    - dict í˜•íƒœ {value, confidence}ë©´ confidence>=0.7 ì¼ ë•Œë§Œ ë°˜ì˜
    - ì¤‘ìœ„ì†Œë“ ë¹„ìœ¨(income_median_ratio/median_income_ratio)ì€
      _parse_median_income_ratio ë¥¼ í†µí•´ ìˆ«ìë¡œ ì •ê·œí™”í•´ì„œ
      profiles.median_income_ratio ì»¬ëŸ¼ì—ë§Œ ì €ì¥í•œë‹¤.
    """
    merged: Dict[str, Any] = dict(db_profile or {})
    changes = 0

    # 0) ê¸°ì¡´ DBì— ë¬¸ìì—´ë¡œ ë“¤ì–´ìˆì„ ìˆ˜ë„ ìˆëŠ” median_income_ratio ë°©ì–´ì ìœ¼ë¡œ ì •ê·œí™”
    existing_mir = merged.get("median_income_ratio")
    if isinstance(existing_mir, str):
        parsed = _parse_median_income_ratio(existing_mir)
        if parsed is not None:
            merged["median_income_ratio"] = parsed

    eph = dict(ephemeral or {})

    # 1) ì¤‘ìœ„ì†Œë“ ë¹„ìœ¨ íŠ¹ìˆ˜ ì²˜ë¦¬
    #    - ephemeral["income_median_ratio"] ë˜ëŠ” ["median_income_ratio"] ì¤‘ í•˜ë‚˜ ì‚¬ìš©
    raw_ratio_field = eph.get("income_median_ratio")
    if raw_ratio_field in (None, "", [], {}):
        raw_ratio_field = eph.get("median_income_ratio")

    if raw_ratio_field not in (None, "", [], {}):
        conf = 1.0
        raw_val = raw_ratio_field
        if isinstance(raw_ratio_field, dict) and "value" in raw_ratio_field and "confidence" in raw_ratio_field:
            conf = float(raw_ratio_field.get("confidence", 1.0))
            raw_val = raw_ratio_field.get("value")

        if conf >= 0.7:
            parsed = _parse_median_income_ratio(raw_val)
            if parsed is not None and merged.get("median_income_ratio") != parsed:
                merged["median_income_ratio"] = parsed
                changes += 1

    # 2) ë‚˜ë¨¸ì§€ í•„ë“œ ì¼ë°˜ ì²˜ë¦¬ (income/median ê´€ë ¨ í‚¤ëŠ” ìŠ¤í‚µ)
    for k, v in eph.items():
        if k in ("income_median_ratio", "median_income_ratio"):
            continue  # ìœ„ì—ì„œ ë³„ë„ ì²˜ë¦¬í–ˆìŒ

        if v in (None, "", [], {}):
            continue

        conf = 1.0
        if isinstance(v, dict) and "value" in v and "confidence" in v:
            conf = float(v.get("confidence", 1.0))
            v = v.get("value")

        if conf < 0.7:
            continue

        if merged.get(k) != v:
            merged[k] = v
            changes += 1

    merged["updated_at"] = datetime.now(timezone.utc)
    merged["_merge_changes"] = changes
    return merged


def _merge_collection(ephemeral: Any, db_coll: Optional[List[Dict[str, Any]]]) -> Dict[str, Any]:
    """
    ì»¬ë ‰ì…˜ ë³‘í•© (íŠ¸ë¦¬í”Œ ê¸°ë°˜):

    - db_coll: DBì—ì„œ ì½ì–´ì˜¨ ê¸°ì¡´ rows (list[dict])
      ê° dictëŠ” {"id", "profile_id", "subject", "predicate", "object", "code_system", "code"}ë¥¼ ê°€ì •
    - ephemeral:
        * dict í˜•íƒœ: {"triples": [ {...}, ... ]}
        * ë˜ëŠ” list í˜•íƒœ: [ {...}, ... ] ë¡œ ë“¤ì–´ì˜¨ ê²½ìš°ë„ í—ˆìš©

    ë°˜í™˜: {
      "triples": [merged_triples...],
      "_merge_changes": int(ìƒˆë¡œ ì¶”ê°€ëœ íŠ¸ë¦¬í”Œ ìˆ˜)
    }
    """
    existing_triples: List[Dict[str, Any]] = list(db_coll or [])
    merged: List[Dict[str, Any]] = list(existing_triples)

    # ê¸°ì¡´ í‚¤ ì§‘í•© (profile_idëŠ” upsert ì‹œ ì™¸ë¶€ì—ì„œ ë„£ìŒ)
    existing_keys = set()
    for t in existing_triples:
        key = (
            (t.get("subject") or "").strip(),
            (t.get("predicate") or "").strip(),
            (t.get("object") or "").strip(),
            (t.get("code_system") or "") or "",
            (t.get("code") or "") or "",
        )
        existing_keys.add(key)

    # ephemeralì—ì„œ ìƒˆ íŠ¸ë¦¬í”Œ í›„ë³´ ê°€ì ¸ì˜¤ê¸°
    new_triples: List[Dict[str, Any]] = []
    if isinstance(ephemeral, dict):
        triples_from_dict = ephemeral.get("triples")
        if isinstance(triples_from_dict, list):
            new_triples = list(triples_from_dict)
    elif isinstance(ephemeral, list):
        new_triples = list(ephemeral)

    changes = 0
    for t in new_triples:
        subj = (t.get("subject") or "").strip()
        pred = (t.get("predicate") or "").strip()
        obj = (t.get("object") or "").strip()
        cs = (t.get("code_system") or "") or None
        cd = (t.get("code") or "") or None

        if not subj or not pred or not obj:
            continue

        key = (subj, pred, obj, cs or "", cd or "")
        if key in existing_keys:
            continue

        existing_keys.add(key)
        merged.append({
            "subject": subj,
            "predicate": pred,
            "object": obj,
            "code_system": cs,
            "code": cd,
        })
        changes += 1

    return {
        "triples": merged,
        "_merge_changes": changes,
    }


def _diff_merge(cur, state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì „ì²´ ë³‘í•© íŒŒì´í”„ë¼ì¸:
      - DBì—ì„œ profile/collection ì¡°íšŒ
      - ephemeral_* ê³¼ ë³‘í•©
    """
    profile_id = state.get("profile_id")
    if profile_id is None:
        # í”„ë¡œí•„ì´ ì—†ìœ¼ë©´ ë³‘í•© ìƒëµ
        return {
            "merged_profile": None,
            "merged_collection": None,
            "merge_log": ["no profile_id; skip merge"],
        }

    eprof = dict(state.get("ephemeral_profile") or {})
    ecoll = state.get("ephemeral_collection") or {}

    db_prof = db_user_utils.get_profile_by_id(cur, profile_id)
    db_coll = db_user_utils.get_collection_by_profile(cur, profile_id)

    merged_prof = _merge_profile(eprof, db_prof)
    merged_coll = _merge_collection(ecoll, db_coll)

    merge_log = []
    if merged_prof.get("_merge_changes"):
        merge_log.append(f"profile: {merged_prof['_merge_changes']} fields updated")
    if merged_coll.get("_merge_changes"):
        merge_log.append(f"collection: {merged_coll['_merge_changes']} triples added")

    return {
        "merged_profile": merged_prof,
        "merged_collection": merged_coll,
        "merge_log": merge_log,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸: persist ë…¸ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def persist(
    state: Dict[str, Any],
    *,
    enable_cleaner: Optional[bool] = None,
    cleaner_mode: Optional[Literal["full", "mask-only", "off"]] = None,
    no_store_policy: Optional[Literal["drop", "redact"]] = None,
) -> Dict[str, Any]:
    """
    LangGraph ë…¸ë“œ: ì„¸ì…˜ ì¢…ë£Œ ì‹œ í˜¸ì¶œ.
    - Cleaner ë™ì‘ì€ (ì¸ì) > (í™˜ê²½ë³€ìˆ˜) ìˆœìœ¼ë¡œ ê²°ì •.
    - DB upsertëŠ” psycopg íŠ¸ëœì­ì…˜ ì•ˆì—ì„œ ìˆ˜í–‰.

    ì¤‘ìš”:
      - state["messages"]ëŠ” ê·¸ë˜í”„ ì „ì²´ì—ì„œ append-onlyë¡œ ê´€ë¦¬ë˜ë¯€ë¡œ,
        ì—¬ê¸°ì„œëŠ” ê·¸ ì „ì²´ë¥¼ ì½ì–´ DBì— ì €ì¥ë§Œ í•˜ê³ ,
        ê·¸ë˜í”„ì— ë˜ëŒë ¤ì¤„ "messages"ëŠ” ì´ë²ˆ ë…¸ë“œì—ì„œ ìƒˆë¡œ ë‚¨ê¸´ tool ë¡œê·¸(delta)ë§Œ ë¦¬í„´í•œë‹¤.
    """
    # DB URL ì—†ìœ¼ë©´ DB ì‘ì—…ì„ ìŠ¤í‚µí•˜ê³  ë¡œê·¸ë§Œ ë‚¨ê¹€
    if not DB_URL:
        raw_msgs: List[Message] = list(state.get("messages") or [])
        msgs_for_db = raw_msgs  # ê·¸ëŒ€ë¡œ ì‚¬ìš© (cleanerë„ ì•ˆ ë“¤ì–´ê°)
        # delta ìš© ë¡œê·¸
        log_msg = _append_tool(
            msgs_for_db,
            "[persist_pipeline] DATABASE_URL not set; skipping DB upsert",
        )
        result: PersistResult = {
            "ok": False,
            "conversation_id": None,
            "counts": {"messages": len(msgs_for_db), "embeddings": 0},
            "warnings": ["DATABASE_URL not set"],
        }
        return {
            "messages": [log_msg],  # deltaë§Œ ë¦¬í„´
            "persist_result": result,
            "rolling_summary": state.get("rolling_summary"),
        }

    # ê·¸ë˜í”„ stateì—ì„œ messages ì „ì²´ë¥¼ ì½ì–´ì„œ DBì— ì €ì¥ìš©ìœ¼ë¡œ ì‚¬ìš©
    raw_msgs: List[Message] = list(state.get("messages") or [])
    rolling_summary = state.get("rolling_summary")
    profile_id = state.get("profile_id")

    # 1) Cleaner í† ê¸€ íŒŒë¼ë¯¸í„° ê²°ì •
    _enable = ENV_ENABLE if enable_cleaner is None else bool(enable_cleaner)
    _mode = ENV_MODE if cleaner_mode is None else cleaner_mode
    _no_store = ENV_NO_STORE_POLICY if no_store_policy is None else no_store_policy

    # 2) state.messages ë‚´ì—ì„œ ì¤‘ë³µ ì œê±° (content + role ê¸°ì¤€)
    #    â†’ LLMì€ ì „ì²´ ëŒ€í™” ì´ë ¥ì„ ì°¸ì¡°í•˜ì§€ë§Œ, DBì—ëŠ” ì¤‘ë³µ ì—†ì´ ì €ì¥
    seen = set()
    deduped_msgs: List[Message] = []
    for m in raw_msgs:
        key = (m.get("content", ""), m.get("role", ""))
        if key not in seen:
            seen.add(key)
            deduped_msgs.append(m)

    # 3) ë©”ì‹œì§€ í´ë¦¬ë‹ (PII ë§ˆìŠ¤í‚¹, no_store ì²˜ë¦¬, ê¸¸ì´ ì œí•œ)
    cleaned: List[Message] = clean_messages(
        messages=deduped_msgs,  # ì¤‘ë³µ ì œê±°ëœ ë©”ì‹œì§€ ì‚¬ìš©
        enable=_enable,
        mode=_mode,
        no_store_policy=_no_store,
    )

    # delta ë¡œ ë°˜í™˜í•  tool ë¡œê·¸ë“¤ì€ ë”°ë¡œ ëª¨ì€ë‹¤.
    log_messages: List[Message] = []

    # cleaner ì ìš© ë¡œê·¸ëŠ” cleanedì—ë„(ì‹¤ì œ DB ì €ì¥ìš©) ë‚¨ê¸°ê³ ,
    # ë°˜í™˜ delta(log_messages)ì—ë„ ê³µìœ í•œë‹¤.
    log_messages.append(
        _append_tool(
            cleaned,
            "[persist_pipeline] cleaner applied",
            {"enable": _enable, "mode": _mode, "no_store_policy": _no_store},
        )
    )

    # 3) ìµœì¢… ìš”ì•½ ìƒì„±
    final_summary = _summarize_session(rolling_summary, cleaned)

    # 4) ì„ë² ë”© (bge-m3-ko)
    embeddings = _embed_chunks(final_summary)

    # 5) DB upsert (íŠ¸ëœì­ì…˜)
    warnings: List[str] = []
    conversation_id: Optional[str] = None
    msg_count = len(cleaned)
    emb_count = len(embeddings)

    try:
        with psycopg.connect(DB_URL, autocommit=False) as conn:
            with conn.cursor() as cur:
                merged_profile = None
                merged_collection = None
                merge_log: List[str] = []

                # 5-1) profile / collections ë³‘í•© + upsert
                if profile_id is not None:
                    merge_result = _diff_merge(cur, state)
                    merged_profile = merge_result.get("merged_profile")
                    merged_collection = merge_result.get("merged_collection")
                    merge_log = merge_result.get("merge_log") or []

                    log_messages.append(
                        _append_tool(
                            cleaned,
                            "[persist_pipeline] diff_merge completed",
                            {"log": merge_log},
                        )
                    )

                    # profiles upsert
                    if merged_profile is not None:
                        pid = db_user_utils.upsert_profile(cur, merged_profile)
                        profile_id = pid  # ìƒˆë¡œ ìƒì„±ëì„ ê²½ìš° ê°±ì‹ 

                    # collections upsert (íŠ¸ë¦¬í”Œ ê¸°ë°˜)
                    if merged_collection is not None:
                        triples = merged_collection.get("triples") or []
                        db_user_utils.upsert_collection(cur, profile_id, triples)

                else:
                    warnings.append("profile_id is None; skip profile/collection upsert")
                    log_messages.append(
                        _append_tool(
                            cleaned,
                            "[persist_pipeline] no profile_id; skip profile/collection",
                        )
                    )

                # 5-2) conversations upsert
                summary_obj: Dict[str, Any] = {"text": final_summary}
                model_stats = state.get("model_stats") or {}
                if profile_id is not None:
                    conversation_id = db_user_utils.upsert_conversation(
                        cur,
                        profile_id=profile_id,
                        summary=summary_obj,
                        model_stats=model_stats,
                        ended_at=datetime.now(timezone.utc),
                    )
                else:
                    warnings.append("conversation not saved: profile_id is None")

                # 5-3) messages / embeddings insert
                if conversation_id is not None:
                    db_user_utils.bulk_insert_messages(cur, conversation_id, cleaned)
                    if embeddings:
                        db_user_utils.bulk_insert_conversation_embeddings(cur, conversation_id, embeddings)

                conn.commit()

    except Exception as e:
        warnings.append(f"DB error: {e}")
        log_messages.append(
            _append_tool(
                cleaned,
                "[persist_pipeline] DB error; rollback",
                {"error": str(e)},
            )
        )

    # 6) ê²°ê³¼ ë¦¬í„´
    result: PersistResult = {
        "ok": len(warnings) == 0,
        "conversation_id": conversation_id,
        "counts": {"messages": msg_count, "embeddings": emb_count},
        "warnings": warnings,
    }

    log_messages.append(
        _append_tool(
            cleaned,
            "[persist_pipeline] done",
            {
                "ok": result["ok"],
                "conversation_id": conversation_id,
                "counts": result["counts"],
                "warnings": warnings,
            },
        )
    )

    return {
        # ğŸ”¹ ê·¸ë˜í”„ì—ëŠ” ì´ë²ˆ ë…¸ë“œì—ì„œ ìƒˆë¡œ ìƒì„±í•œ tool ë¡œê·¸(delta)ë§Œ ë„˜ê¸´ë‹¤.
        "messages": log_messages,
        "persist_result": result,
        "rolling_summary": final_summary,
        "profile_id": profile_id,
    }
