# llm_answer_creator.py (Gemini Version)
# ëª©ì : "Answer LLM" ë…¸ë“œ
# - RetrievalPlannerì˜ ê²°ê³¼ë¥¼ ë°›ì•„ ìµœì¢… ë‹µë³€ ìƒì„±
# - Google Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±

from __future__ import annotations

import json
import os
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
import google.generativeai as genai

from app.langgraph.state.ephemeral_context import State as GraphState, Message

load_dotenv()

# Gemini API ì„¤ì •
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
ANSWER_MODEL = os.getenv("ANSWER_MODEL", "gemini-2.0-flash")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SYSTEM_PROMPT = """
# ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” RetrievalPlannerë¡œë¶€í„° ì „ë‹¬ëœ ë¬¸ì„œ ëª©ë¡ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
# ê·œì¹™:
# - ì „ë‹¬ëœ ë¬¸ì„œë“¤ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
# - ì „ë‹¬ë˜ì§€ ì•Šì€ ë¬¸ì„œëŠ” ìƒì„±í•˜ê±°ë‚˜ ê°€ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# - ì „ë‹¬ëœ ë¬¸ì„œê°€ 6ê°œë©´ 6ê°œ ëª¨ë‘ ì¶œë ¥í•˜ê³ ,
#   ì „ë‹¬ëœ ë¬¸ì„œê°€ 1ê°œë©´ 1ê°œë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
# - ì‚¬ìš©ìê°€ ìê²©ì´ ë˜ëŠ” ì§€ì›ì‚¬ì—…ë§Œ ì´ë¯¸ í•„í„°ë§ëœ ìƒíƒœë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
# - ë‹¹ì‹ ì€ ì¶”ê°€ì ì¸ ìê²© íŒë‹¨ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# - ë¬¸ì„œì— ìˆëŠ” ìš”ê±´ ë° ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½í•˜ì—¬ ì•ˆë‚´í•©ë‹ˆë‹¤.
# - ë‹µë³€ ë§ˆì§€ë§‰ì— ì¶œì²˜ URLì„ í¬í•¨í•©ë‹ˆë‹¤.
# """

SYSTEM_PROMPT = """
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” RetrievalPlannerë¡œë¶€í„° ì „ë‹¬ëœ ë¬¸ì„œ ëª©ë¡ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ê·œì¹™(ì ˆëŒ€ ì¤€ìˆ˜):
- ì „ë‹¬ëœ ë¬¸ì„œë“¤ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
- ì „ë‹¬ë˜ì§€ ì•Šì€ ë¬¸ì„œëŠ” ìƒì„±í•˜ê±°ë‚˜ ì¶”ë¡ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì „ë‹¬ëœ document ê°œìˆ˜ë§Œí¼ ì •í™•íˆ ê°™ì€ ê°œìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
- ì´ë¯¸ RetrievalPlannerì—ì„œ ìê²© í•„í„°ë§ì´ ì™„ë£Œëœ ìƒíƒœì´ë¯€ë¡œ ì¶”ê°€ ìê²© íŒë‹¨ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹(ê°•ì œ):
ê° ë¬¸ì„œëŠ” ì•„ë˜ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤:

{ë¬¸ì„œë²ˆí˜¸}. {title}
- ì§€ì› ë‚´ìš©: ë¬¸ì„œì˜ "benefits" ë˜ëŠ” snippet ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½
- ì§€ì› ìê²©: ë¬¸ì„œì˜ "requirements" ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½
- ì‹ ì²­ ë°©ë²•: ë¬¸ì„œì— ì¡´ì¬í•˜ë©´ ìš”ì•½, ì—†ìœ¼ë©´ ë§í¬ ì°¸ì¡°
- ë§í¬: {url}

ì£¼ì˜:
- ë§í¬ëŠ” ê° ë¬¸ì„œë§ˆë‹¤ ë”± í•œ ë²ˆë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
- ë§ˆì§€ë§‰ì— ì „ì²´ URL ëª©ë¡ì„ ë‹¤ì‹œ ë‚˜ì—´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì§€ì› ë‚´ìš©/ìê²©/ì‹ ì²­ë°©ë²•ì´ ë¬¸ì„œì— ì—†ìœ¼ë©´ "ì œê³µëœ ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…ì‹œí•©ë‹ˆë‹¤.
- ë¬¸ì„œ ìˆœì„œëŠ” ì „ë‹¬ë°›ì€ ìˆœì„œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

ë‹µë³€ ì „ì²´ êµ¬ì¡°:
1) ê°„ë‹¨í•œ í•œ ì¤„ ê²°ë¡ 
2) ìœ„ ì¶œë ¥ í˜•ì‹ì— ë”°ë¼ ë¬¸ì„œë“¤ì„ ë‚˜ì—´
3) ì¶”ê°€ ì•ˆë‚´(í•„ìš”í•œ ê²½ìš°ë§Œ)
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¨í…ìŠ¤íŠ¸ ìš”ì•½/ì„œì‹í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _format_profile_ctx(p: Optional[Dict[str, Any]]) -> str:
    if not p or "error" in p:
        return ""
    lines: List[str] = []

    if p.get("summary"):
        lines.append(f"- ìš”ì•½: {p['summary']}")

    if p.get("insurance_type"):
        lines.append(f"- ê±´ë³´ ìê²©: {p['insurance_type']}")

    mir_raw = p.get("median_income_ratio")
    if mir_raw is not None:
        try:
            v = float(mir_raw)
            if v <= 10:
                pct = v * 100.0
            else:
                pct = v
            lines.append(f"- ì¤‘ìœ„ì†Œë“ ë¹„ìœ¨: {pct:.1f}%")
        except:
            lines.append(f"- ì¤‘ìœ„ì†Œë“ ë¹„ìœ¨: {mir_raw}")

    if (bb := p.get("basic_benefit_type")):
        lines.append(f"- ê¸°ì´ˆìƒí™œë³´ì¥: {bb}")

    if (dg := p.get("disability_grade")) is not None:
        dg_label = {0: "ë¯¸ë“±ë¡", 1: "ì‹¬í•œ", 2: "ì‹¬í•˜ì§€ì•ŠìŒ"}.get(dg, str(dg))
        lines.append(f"- ì¥ì•  ë“±ê¸‰: {dg_label}")

    if (lt := p.get("ltci_grade")) and lt != "NONE":
        lines.append(f"- ì¥ê¸°ìš”ì–‘ ë“±ê¸‰: {lt}")

    if p.get("pregnant_or_postpartum12m") is True:
        lines.append("- ì„ì‹ /ì¶œì‚° 12ê°œì›” ì´ë‚´")

    return "\n".join(lines)


def _format_collection_ctx(items: Optional[List[Dict[str, Any]]]) -> str:
    if not items:
        return ""
    out = []
    for it in items[:8]:
        if "error" in it:
            continue
        segs = []
        if it.get("predicate"):
            segs.append(f"[{it['predicate']}]")
        if it.get("object"):
            segs.append(it["object"])
        out.append("- " + " ".join(segs))
    return "\n".join(out)


def _format_documents(items: Optional[List[Dict[str, Any]]]) -> str:
    if not items:
        return ""
    out: List[str] = []

    for idx, doc in enumerate(items[:6], start=1):
        if not isinstance(doc, dict):
            continue

        title = doc.get("title") or doc.get("doc_id") or f"ë¬¸ì„œ {idx}"
        source = doc.get("source")
        score = doc.get("score")
        url = doc.get("url")
        snippet = doc.get("snippet") or ""

        header = f"{idx}. {title}"
        if source:
            header += f" ({source})"
        if score:
            header += f" [score={score:.3f}]"

        out.append(f"- {header}")
        out.append(f"  > {snippet.strip()}")

        if url:
            out.append(f"  ì¶œì²˜: {url}")

    return "\n".join(out)


def _build_user_prompt(
    input_text: str,
    used: str,
    profile_ctx: Optional[Dict[str, Any]],
    collection_ctx: Optional[List[Dict[str, Any]]],
    summary: Optional[str] = None,
    documents: Optional[List[Dict[str, Any]]] = None,
) -> str:
    prof_block = _format_profile_ctx(profile_ctx)
    coll_block = _format_collection_ctx(collection_ctx)
    doc_block = _format_documents(documents)
    summary_block = (summary or "").strip()

    lines = [f"ì‚¬ìš©ì ì§ˆë¬¸:\n{input_text.strip()}"]
    lines.append(f"\n[Retrieval ì‚¬ìš©: {used}]")

    if prof_block:
        lines.append("\n[Profile ì»¨í…ìŠ¤íŠ¸]\n" + prof_block)
    if coll_block:
        lines.append("\n[Collection ì»¨í…ìŠ¤íŠ¸]\n" + coll_block)
    if summary_block:
        lines.append("\n[Rolling Summary]\n" + summary_block)
    if doc_block:
        lines.append("\n[RAG ë¬¸ì„œ ìŠ¤ë‹ˆí«]\n" + doc_block)

    lines.append("""
ìš”êµ¬ ì¶œë ¥:
- ë§¨ ì•ì— **ê²°ë¡  í•œ ë¬¸ì¥**
- ë‹¤ìŒì— ê·¼ê±°(ìœ„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì¸ìš©)
- ë§ˆì§€ë§‰ì— ë‹¤ìŒ ë‹¨ê³„(ì¦ë¹™, ì¶”ê°€ í™•ì¸, ì‹ ì²­ ê²½ë¡œ)ë¥¼ ê°„ë‹¨íˆ
- ì¶”ì • ê¸ˆì§€, ì»¨í…ìŠ¤íŠ¸ ë°– ì‚¬ì‹¤ ê¸ˆì§€
""")
    return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Gemini LLM í˜¸ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_answer_llm(
    input_text: str,
    used: str,
    profile_ctx: Optional[Dict[str, Any]],
    collection_ctx: Optional[List[Dict[str, Any]]],
    summary: Optional[str] = None,
    documents: Optional[List[Dict[str, Any]]] = None,
) -> str:

    user_prompt = _build_user_prompt(
        input_text,
        used,
        profile_ctx,
        collection_ctx,
        summary=summary,
        documents=documents,
    )

    model = genai.GenerativeModel(ANSWER_MODEL)

    # Gemini 2.x ì—ì„œëŠ” system role ë¶ˆê°€ëŠ¥ â†’ system í”„ë¡¬í”„íŠ¸ë¥¼ ë¬¸ìì—´ ê²°í•©ìœ¼ë¡œ ë„£ì–´ì•¼ í•¨
    full_prompt = SYSTEM_PROMPT + "\n\n" + user_prompt

    try:
        resp = model.generate_content(
            full_prompt,
            generation_config={"temperature": 0.3},
        )

        # 1) resp.textê°€ ìˆì„ ê²½ìš°
        if hasattr(resp, "text") and resp.text:
            return resp.text.strip()

        # 2) Gemini 2.x í‘œì¤€ êµ¬ì¡°: candidates[].content.parts[].text
        if resp.candidates:
            cand = resp.candidates[0]
            if cand.content and cand.content.parts:
                text = "".join(
                    part.text
                    for part in cand.content.parts
                    if hasattr(part, "text")
                )
                return text.strip()

        return str(resp)

    except Exception as e:
        print("ğŸ”¥ğŸ”¥ [Gemini ERROR]", e)
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _extract_context_from_messages(messages: List[Message]) -> Dict[str, Any]:
    for msg in reversed(messages or []):
        if msg.get("role") != "tool":
            continue
        if msg.get("content") != "[context_assembler] prompt_ready":
            continue
        meta = msg.get("meta") or {}
        ctx = meta.get("context")
        if isinstance(ctx, dict):
            return ctx
    return {}


def _last_user_content(messages: List[Message]) -> str:
    for msg in reversed(messages or []):
        if msg.get("role") == "user":
            return msg.get("content", "")
    return ""


def _infer_used_flag(profile_ctx: Any, collection_ctx: Any, documents: Any) -> str:
    has_profile = isinstance(profile_ctx, dict) and bool(profile_ctx)
    has_collection = isinstance(collection_ctx, list) and bool(collection_ctx)
    has_docs = isinstance(documents, list) and bool(documents)
    if has_profile and (has_collection or has_docs):
        return "BOTH"
    if has_profile:
        return "PROFILE"
    if has_collection or has_docs:
        return "COLLECTION"
    return "NONE"


def _safe_json(value: Any, limit: int = 400) -> str:
    if not value:
        return "ì—†ìŒ"
    try:
        text = json.dumps(value, ensure_ascii=False)
    except Exception:
        text = str(value)
    return text[:limit] + ("..." if len(text) > limit else "")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Fallback ë©”ì‹œì§€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _build_fallback_text(
    used: str,
    profile_ctx: Any,
    collection_ctx: Any,
    documents: Any,
    summary: Optional[str],
) -> str:
    return (
        "ì£„ì†¡í•´ìš”. ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”.\n\n"
        "## ê·¼ê±°(ìš”ì•½)\n"
        f"- Retrieval ì‚¬ìš©: {used}\n"
        f"- Summary: {(summary or 'ì—†ìŒ')[:400]}\n"
        f"- Profile: {_safe_json(profile_ctx)}\n"
        f"- Collection: {_safe_json(collection_ctx)}\n"
        f"- Documents: {_safe_json(documents)}\n"
        "í•„ìš” ì‹œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ answer ë…¸ë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def answer(state: GraphState) -> Dict[str, Any]:
    messages: List[Message] = list(state.get("messages") or [])
    retrieval = state.get("retrieval") or {}
    ctx = _extract_context_from_messages(messages)

    profile_ctx = ctx.get("profile") or retrieval.get("profile_ctx")
    collection_ctx = ctx.get("collection") or retrieval.get("collection_ctx")

    if isinstance(collection_ctx, dict) and "triples" in collection_ctx:
        collection_ctx_list = collection_ctx["triples"]
    elif isinstance(collection_ctx, list):
        collection_ctx_list = collection_ctx
    else:
        collection_ctx_list = None

    documents = ctx.get("documents") or retrieval.get("rag_snippets")
    summary = ctx.get("summary") or state.get("rolling_summary")

    input_text = (
        (state.get("user_input") or state.get("input_text") or "").strip()
        or _last_user_content(messages).strip()
    )

    used = (retrieval.get("used") or "").strip().upper()
    if not used:
        used = _infer_used_flag(profile_ctx, collection_ctx_list, documents)

    try:
        text = run_answer_llm(
            input_text,
            used,
            profile_ctx,
            collection_ctx_list,
            summary=summary,
            documents=documents,
        )
    except Exception:
        text = _build_fallback_text(
            used,
            profile_ctx,
            collection_ctx_list,
            documents,
            summary,
        )

    citations = {
        "profile": profile_ctx,
        "collection": collection_ctx_list,
        "documents": documents,
    }

    assistant_message: Message = {
        "role": "assistant",
        "content": text,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "meta": {
            "model": ANSWER_MODEL,
            "used": used,
            "citations": {
                "profile": bool(profile_ctx),
                "collection_count": len(collection_ctx_list or []),
                "document_count": len(documents or []),
            },
        },
    }

    return {
        "answer": {
            "text": text,
            "citations": citations,
            "used": used,
        },
        "messages": [assistant_message],
    }


def answer_llm_node(state: GraphState) -> Dict[str, Any]:
    return answer(state)
